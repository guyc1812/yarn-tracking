# YARN 任务跟踪探明 (一)

## **主线：准备一块敲门砖: Word Count Demo**

* [书写并提交一个 Spark 应用](./1.&#32;Demo.md#1)
* [项目准备](./1.&#32;Demo.md#2)
* [项目介绍](./1.&#32;Demo.md#3)
* [Demo 涉及到的 Spark 配置](./1.&#32;Demo.md#4)
* [关于 Spark Event Log](./1.&#32;Demo.md#5)
* [其他说明](./1.&#32;Demo.md#6)
* [相关链接](./1.&#32;Demo.md#7)

<br><h3 id="1"><b>书写并提交一个 Spark 应用</b></h3>

本篇将准备一个 Word Count 程序，作为整个系列的演示，Demo 将完成以下动作：
1. 对接一个 Hadoop 集群
2. 配置一个 Spark 任务 (Work Count)
3. 向 YARN 提交这个 Spark 应用程序

<br><h3 id="2"><b>项目准备</b></h3>

   * 集群配置文件：`hadoop-conf`，此文件夹标记为 `test-resources`
   * 静态文件资源：
      * 本地 Jar 包
         1. `src/test/resources/jar-files/word-count-demo.jar`，项目编译好之后将 Jar 包复制到 resources 目录下，作为 Spark 应用的可执行文件
         2. `src/test/resources/jar-files/gson-2.8.0.jar`，第三方依赖包，此处只为了演示 Spark 携带本地依赖文件
      * 本地配置文件，此处以 log4j 配置文件为例
         1. `src/test/resources/log-files/log4j-info.properties`
         2. `src/test/resources/log-files/log4j-warn.properties`
         3. `src/test/resources/log-files/log4j-debug.properties`
   * `com._4pd.spark.SparkSubmitter`，自定义 Spark 应用的提交器
   * `com._4pd.spark.WordCount.scala`，Spark 应用中真正执行的计算逻辑，打包成 word-count-demo.jar
   * `com._4pd.spark.Demo`，演示类，调用提交器完成 Spark 应用的提交
```
.
├── hadoop-conf
│   ├── core-site.xml
│   └── yarn-site.xml
│   ├── hdfs-site.xml
│   ├── mapred-site.xml
│   ├── log4j.properties
├── src
│   ├── main
│   │   ├── java
│   │   │   └── com._4pd.spark.SparkSubmitter
│   │   └── scala
│   │       └── com._4pd.spark.WordCount.scala
│   └── test
│       ├── java
│       │   └── com._4pd.spark.Demo
│       └── resources
│           ├── jar-files
│           │   ├── gson-2.8.0.jar
│           │   └── word-count-demo.jar
│           └── log-files
│               ├── log4j-debug.properties
│               ├── log4j-info.properties
│               └── log4j-warn.properties
└── pom.xml
```

<br><h3 id="3"><b>项目介绍</b></h3>

1. 准备 Word Count 示例程序，程序中打印 Spark 应用信息和系统变量
    ```scala
    package com._4pd.spark

    import org.apache.spark.sql.SparkSession

    object WordCount {

        def main(args: Array[String]) {

            // 初始化 Spark Session
            val spark = SparkSession
                .builder
                .appName("Work Count Demo")
                .getOrCreate()

            // 打印 Spark 应用配置信息及系统变量
            envLogger(spark)
            
            // 准备 Demo 数据
            val words = spark.sparkContext.parallelize(
                Array("hello world",
                      "hello hadoop",
                      "hello spark",
                      "hadoop nice",
                      "spark nice")
                ).cache()

            // Word Count
            val counts = words
                .flatMap(line => line.split(" "))
                .map(word => (word, 1))
                .reduceByKey(_ + _)
            
            // 隐式转换
            import spark.implicits._
            counts.toDF.show()

            spark.stop()
        }

        def envLogger(spark: SparkSession): Unit = {
            import scala.collection.JavaConversions._
            println("SPARK VERSION ============>")
            println(spark.version)
            println("SPARK CONFIGS ============>")
            println(spark.sparkContext.getConf.toDebugString)
            println("ENV ======================>")
            for ((k, v) <- System.getenv) println(s"$k = $v")
            println("PROPERTIES ===============>")
            System.getProperties.list(System.out)
        }

    }
    ```
2. 准备 Spark 任务提交器，用于接收 Spark 应用的配置和参数，完成提交应用到 YARN 的动作
    ```java
    package com._4pd.spark;

    import lombok.extern.slf4j.Slf4j;
    import org.apache.hadoop.yarn.api.records.ApplicationId;
    import org.apache.hadoop.yarn.api.records.ApplicationReport;
    import org.apache.spark.SparkConf;
    import org.apache.spark.deploy.yarn.Client;
    import org.apache.spark.deploy.yarn.ClientArguments;

    @Slf4j
    class SparkSubmitter {

        /**
        * 用于提交 Spark 应用，需传入 Spark 应用的参数及配置，详见: xxx
        *
        * @param sparkArgs，Spark 应用参数
        * @param sparkConf，Spark 应用配置
        */
        void submitApp(String[] sparkArgs, SparkConf sparkConf) {
            // 构造 ClientArguments 用于提交 Spark 应用
            ClientArguments cArgs = new ClientArguments(sparkArgs);
            // 使用 spark-yarn 提供的 Client 提交 Spark 应用
            Client client = new Client(cArgs, sparkConf);
            // 完成提交动作
            ApplicationId applicationId = client.submitApplication();
            // 获取实时报告
            ApplicationReport report = client.getApplicationReport(applicationId);
            // LOG
            log.info("Application: {} submitted, tracking url is {}.",
                    applicationId, report.getTrackingUrl());
        }

    }
    ```

3. 准备 Demo 演示程序，用于配置 Spark 应用并通过 SparkSubmitter 提交到 YARN 集群
    ```java
    package com._4pd.spark;

    import org.apache.spark.SparkConf;

    public class Demo {

        public static void main(String[] args) {

            String[] sparkArgs = {
                    "--class", "com._4pd.spark.WordCount",
                    "--jar", "src/test/resources/jar-files/word-count-demo.jar"
            };

            // 初始化 SparkConf
            SparkConf sparkConf = new SparkConf();
            // 基本信息设置
            sparkConf.set("spark.submit.deployMode", "cluster");    // Required
            sparkConf.set("spark.app.name", "GG");                  // Required
            sparkConf.set("spark.yarn.queue", "default");           // Required
            sparkConf.set("spark.yarn.appMaxAttempts", "1");
            // 文件资源设置
            sparkConf.set("spark.yarn.jars", "hdfs:///tmp/spark-lib/2.3.0/*.jar");  // Required
            sparkConf.set("spark.yarn.dist.jars", "src/test/resources/jar-files/gson-2.8.0.jar");
            sparkConf.set("spark.yarn.dist.files", String.format("%s,%s,%s",
                    "src/test/resources/log-files/log4j-info.properties",
                    "src/test/resources/log-files/log4j-warn.properties",
                    "src/test/resources/log-files/log4j-debug.properties"));
            // 开启 Spark Event Log
            sparkConf.set("spark.eventLog.enabled", "true");
            sparkConf.set("spark.eventLog.dir", "hdfs:///tmp/jobhistory/spark-demo/");
            // JvmOpt 设置
            sparkConf.set("spark.driver.extraJavaOptions", "-Dlog4j.configuration=log4j-debug.properties");
            sparkConf.set("spark.executor.extraJavaOptions", "-Dlog4j.configuration=log4j-debug.properties");
            // 计算资源设置
            sparkConf.set("spark.driver.cores", "1");      
            sparkConf.set("spark.driver.memory", "512m");  
            sparkConf.set("spark.executor.instances", "1");
            sparkConf.set("spark.executor.cores", "1");    
            sparkConf.set("spark.executor.memory", "512m");

            SparkSubmitter.submitApp(sparkArgs, sparkConf);

        }

    }
    ```

4. 运行
    ```bash
    19/10/27 16:16:17 INFO SparkSubmitter: 
    Application: 
        application_1571091072053_54110 submitted, 
    tracking url is 
        http://m7-common-cdh07:8088/proxy/application_1571091072053_54110/.
    ```

<br><h3 id="4"><b>Demo 涉及到的 Spark 配置，按上例使用顺序</b></h3>

| Property Name | Default | Meaning |
|:--|:--|:--|
| spark.submit.deployMode | (none) | The deploy mode of Spark driver program, either "client" or "cluster", Which means to launch driver program locally ("client") or remotely ("cluster") on one of the nodes inside the cluster. |
| spark.app.name | (none) | The name of your application. This will appear in the UI and in log data. |
| spark.yarn.queue | default | The name of the YARN queue to which the application is submitted. |
| spark.yarn.appMaxAttempts | yarn.resourcemanager.am.max-attempts in YARN | The maximum number of attempts that will be made to submit the application. It should be no larger than the global number of max attempts in the YARN configuration. |
| spark.yarn.jars | (none) | List of libraries containing Spark code to distribute to YARN containers. By default, Spark on YARN will use Spark jars installed locally, but the Spark jars can also be in a world-readable location on HDFS. This allows YARN to cache it on nodes so that it doesn't need to be distributed each time an application runs. To point to jars on HDFS, for example, set this configuration to hdfs:///some/path. Globs are allowed. |
| spark.yarn.dist.jars | (none) | Comma-separated list of jars to be placed in the working directory of each executor. |
| spark.yarn.dist.files | (none) | Comma-separated list of files to be placed in the working directory of each executor. |
| spark.eventLog.enabled | false | Whether to log Spark events, useful for reconstructing the Web UI after the application has finished. |
| spark.eventLog.dir | file:///tmp/spark-events | Base directory in which Spark events are logged, if spark.eventLog.enabled is true. Within this base directory, Spark creates a sub-directory for each application, and logs the events specific to the application in this directory. Users may want to set this to a unified location like an HDFS directory so history files can be read by the history server. |
| spark.driver.extraJavaOptions | (none) | A string of extra JVM options to pass to the driver. For instance, GC settings or other logging. Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap size settings can be set with spark.driver.memory in the cluster mode and through the --driver-memory command line option in the client mode. Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-java-options command line option or in your default properties file. |
| spark.executor.extraJavaOptions | (none) | A string of extra JVM options to pass to executors. For instance, GC settings or other logging. Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this option. Spark properties should be set using a SparkConf object or the spark-defaults.conf file used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory. |
| spark.driver.cores | 1 | Number of cores to use for the driver process, only in cluster mode. |
| spark.driver.memory | 1g | Amount of memory to use for the driver process, i.e. where SparkContext is initialized, in MiB unless otherwise specified (e.g. 1g, 2g). Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-memory command line option or in your default properties file. |
| spark.executor.instances | 2 | The number of executors for static allocation. With spark.dynamicAllocation.enabled, the initial set of executors will be at least this large. |
| spark.executor.cores | 1 in YARN mode, all the available cores on the worker in standalone and Mesos coarse-grained modes. | The number of cores to use on each executor. In standalone and Mesos coarse-grained modes, for more detail, see this [description](http://spark.apache.org/docs/2.3.0/spark-standalone.html). |
| spark.executor.memory | 1g | Amount of memory to use per executor process, in MiB unless otherwise specified. (e.g. 2g, 8g). |

更多关于 Spark 2.3 的配置及解释详见官方文档：[Spark Configuration](http://spark.apache.org/docs/2.3.0/configuration.html)

更多关于 Spark on Yarn 的配置及解释详见官方文档：[Running Spark on YARN](http://spark.apache.org/docs/2.3.0/running-on-yarn.html#configuration)

<br><h3 id="5"><b>关于 Spark Event Log</b></h3>

* 功能

    Spark 任务在运行期间，可以通过 Spark UI 看到任务的详细运行过程，包括 DAG 图，Job，Stage，Task 等信息，但是在运行结束之后就无法再查看任务运行历史。此时可以开启 Spark Event Log，将运行历史持久化下来，然后通过 UI 界面查看。

* 使用方式
    1. 开启 eventLog，设置 SparkConf: spark.eventLog.enabled=true
    2. 指定历史记录持久化到任意地址，设置 SparkConf: spark.eventLog.dir=hdfs:///tmp/jobhistory/spark-demo/
    3. 本地启动 History Server
        ```
        hadoop fs -chmod -R 777 hdfs:///tmp/jobhistory/spark-demo/

        SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs:///tmp/jobhistory/spark-demo/ -Dspark.history.ui.port=7777" $SPARK_HOME/sbin/start-history-server.sh
        ```

* 浏览器打开 http://localhost:7777 便可以看到 Spark 任务历史的列表，点击对应任务历史可以查看 Spark 任务运行细节。

<br><h3 id="6"><b><i>其他说明</i></b></h3>

* 本例中使用了 Spark 官方提供的 Client 向 YARN 提交 Spark 应用
* Client 所属包为：[org.apache.spark.deploy.yarn](https://github.com/apache/spark/tree/v2.3.0/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn)
* Client 接收的参数和配置类型分别为 ClientArguments 和 SparkConf
* ClientArguments 在 Spark 1.x 版本和 2.x 版本中使用方式有所变化，移除了一部分参数的设置到 SparkConf 中，详见：[ClientArguments 设置](./2.&#32;ClientArguments.md)

<br><h3 id="7"><b><i>相关链接</i></b></h3>

* [Spark Configuration](http://spark.apache.org/docs/2.3.0/configuration.html)

* [Running Spark on YARN](http://spark.apache.org/docs/2.3.0/running-on-yarn.html#configuration)

* Demo 项目地址：[Spark Demo](https://github.com/guyc1812/yarn-tracking/tree/master/spark-demo)

<br>

---

<br>

下一篇将介绍 Spark Client 是如何完成 Spark 应用的提交动作的。「[传送门](./2.&#32;Client.md)」



### **[回到目录](./README.md)**
