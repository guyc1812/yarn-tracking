# YARN 任务跟踪探明 (二)

<br>

## **支线：ClientArguments 在 Spark 1.x 与 2.x 中的使用变更**

* [Spark 2.0 之后，ClientArguments 只提供五个参数](./2.1&#32;ClientArguments.md#1)
* [ClientArguments 在 Spark 2.x 与 1.x 中的对比](./2.1&#32;ClientArguments.md#2)
* [相关链接](./2.1&#32;ClientArguments.md#3)

<br><h3 id="1"><b>Spark 2.0 之后，ClientArguments 只提供五个参数</b></h3>

```
--jar               # Path to your application's JAR file (required in yarn-cluster mode)
--class             # Name of your application's main class (required)
--primary-py-file   # A main Python file
--primary-r-file    # A main R file
--arg               # Argument to be passed in order to your application's main class.
```

<br><h3 id="2"><b>ClientArguments 在 Spark 2.x 与 1.x 中的对比</b></h3>

|ClientArguments 字段 (Spark 1.x)|ClientArguments 提交参数 (Spark 1.x)|Spark 2.x 中应使用|默认值|解释|
|:--|:--|:--|:--|:--|
| userJar | --jar |  |  |  |
| userClass | --class |  |  |  |
| primaryPyFile | --primary-py-file |  |  |  |
| primaryRFile | --primary-r-file |  |  |  |
| userArgs | --arg (--args is deprecated) |  |  |  |
| appName | --name | spark.app.name | (none) | The name of your application. This will appear in the UI and in log data. |
| amQueue | --queue | spark.yarn.queue | (none) | The name of the YARN queue to which the application is submitted. |
| addJars | --addJars | spark.yarn.dist.jars | (none) | Comma-separated list of jars to be placed in the working directory of each executor. |
| files | --files | spark.yarn.dist.files | (none) | Comma-separated list of files to be placed in the working directory of each executor. |
| archives | --archives | spark.yarn.dist.archives | (none) | Comma separated list of archives to be extracted into the working directory of each executor. |
| pyFiles | --py-files | spark.submit.pyFiles | (none) | Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. Globs are allowed. |
| keytab | --keytab | spark.yarn.keytab | (none) | The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the YARN Application Master via the YARN Distributed Cache, and will be used for renewing the login tickets and the delegation tokens periodically. Equivalent to the --keytab command line argument. (Works also with the "local" master.) |
| principal | --principal | spark.yarn.principal | (none) | Principal to be used to login to KDC, while running on secure clusters. Equivalent to the --principal command line argument. (Works also with the "local" master.) |
| driverCores | --driver-cores | spark.driver.cores | 1 | Number of cores to use for the driver process, only in cluster mode. |
| driverMemory | --driver-memory (--master-memory is deprecated) | spark.driver.memory | 1g | Amount of memory to use for the driver process, i.e. where SparkContext is initialized, in the same format as JVM memory strings with a size unit suffix ("k", "m", "g" or "t") (e.g. 512m, 2g). Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-memory command line option or in your default properties file. |
| numExecutors | --num-executors (--num-workers is deprecated)  | spark.executor.instances | 2 | The number of executors for static allocation. With spark.dynamicAllocation.enabled, the initial set of executors will be at least this large. |
| executorCores | --executor-cores (--worker-cores is deprecated) | spark.executor.cores | 1 in YARN mode, all the available cores on the worker in standalone and Mesos coarse-grained modes | The number of cores to use on each executor. |
| executorMemory | --executor-memory (--worker-memory is deprecated) | spark.executor.memory | 1g | Amount of memory to use per executor process, in the same format as JVM memory strings with a size unit suffix ("k", "m", "g" or "t") (e.g. 512m, 2g). |
| executorMemoryOverhead |  | spark.executor.memoryOverhead | executorMemory * 0.10, with minimum of 384 | The amount of off-heap memory to be allocated per executor, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%). This option is currently supported on YARN and Kubernetes. |
| driverMemOverhead |  | spark.driver.memoryOverhead | driverMemory * 0.10, with minimum of 384 | The amount of off-heap memory to be allocated per driver in cluster mode, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the container size (typically 6-10%). This option is currently supported on YARN and Kubernetes. |
| amMemory |  | spark.yarn.am.memory | 512m | Amount of memory to use for the YARN Application Master in client mode, in the same format as JVM memory strings (e.g. 512m, 2g). In cluster mode, use spark.driver.memory instead.Use lower-case suffixes, e.g. k, m, g, t, and p, for kibi-, mebi-, gibi-, tebi-, and pebibytes, respectively. |
| amCores |  | spark.yarn.am.cores | 1 | Number of cores to use for the YARN Application Master in client mode. In cluster mode, use spark.driver.cores instead. |
| amMemoryOverhead |  | spark.yarn.am.memoryOverhead | AM memory * 0.10, with minimum of 384 | Same as spark.driver.memoryOverhead, but for the YARN Application Master in client mode. |

<br><h3 id="3"><b><i>相关链接</i></b></h3>

[ClientArguments.scala](https://github.com/apache/spark/blob/v2.3.0/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ClientArguments.scala)

<br>

---

<br>

### **[回到目录](./README.md)**